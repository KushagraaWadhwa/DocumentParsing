{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error running pipeline: name 'data' is not defined\n"
     ]
    }
   ],
   "source": [
    "from py_pdf_parser.loaders import load_file\n",
    "from PyPDF2 import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import camelot\n",
    "from llama_parse import LlamaParse\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class TextExtractor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.output_folder = config.get('output_folder', 'output')\n",
    "\n",
    "    def get_text_parsing_module(self):\n",
    "        if self.config['text_parsing'] == 'pypdfparser':\n",
    "            return self.parse_text_with_pypdfparser\n",
    "        elif self.config['text_parsing'] == 'pypdf2':\n",
    "            return self.parse_text_with_pypdf2\n",
    "\n",
    "    def parse_text_with_pypdfparser(self, file_path):\n",
    "        try:\n",
    "            FONT_MAPPING = {\n",
    "                r\"\\w{6}\\+TrebuchetMS-Bold,18\\.0\": \"title\",\n",
    "                r\"\\w{6}\\+TrebuchetMS-Bold,18\\.0\": \"title\",\n",
    "                r\"\\w{6}\\+Calibri-Bold,24\\.0\": \"title\",\n",
    "                r\"\\w{6}\\+Calibri-Bold,28\\.0\": \"title\",\n",
    "                r\"\\w{6}\\+Calibri-Bold,13\\.9\": \"subheading\",\n",
    "                r\"\\w{6}\\+Calibri-Bold,14\\.0\": \"subheading\",\n",
    "                r\"\\w{6}\\+Calibri-Bold,16\\.0\": \"subheading\",\n",
    "                r\"\\w{6}\\+Calibri-Bold,16\\.6\": \"subheading\",\n",
    "                r\"\\w{6}\\+Calibri-Bold,12\\.0\": \"nested_subheading\",\n",
    "                r\"\\w{6}\\+Calibri-Bold,13\\.0\": \"nested_subheading\",\n",
    "                r\"\\w{6}\\+Calibri,12\\.0\": \"content\",\n",
    "                r\"\\w{6}\\+Calibri-BoldItalic,12\\.0\": \"content\",\n",
    "                r\"\\w{6}\\+Calibri-Italic,12\\.0\": \"content\",\n",
    "                r\"\\w{6}\\+Calibri-Italic,11\\.0\": \"content\",\n",
    "                r\"ArialMT,8\\.0\": \"ignored\"\n",
    "            }\n",
    "\n",
    "            filenames = os.listdir(file_path)\n",
    "            for filename in filenames:\n",
    "                x = file_path + \"/\"+filename\n",
    "                document = load_file(\n",
    "                    x, font_mapping=FONT_MAPPING, font_mapping_is_regex=True)\n",
    "                data = {}\n",
    "                title_policy = []\n",
    "                current_subheading = None\n",
    "                current_nested_subheading = None\n",
    "\n",
    "                for element in document.elements:\n",
    "                    try:\n",
    "                        if element.filter_by_font(\"ignored\"):\n",
    "                            continue\n",
    "                        elif element.filter_by_font(\"title\"):\n",
    "                            title_policy.append(element.text().strip())\n",
    "                        elif element.filter_by_font(\"subheading\"):\n",
    "                            current_subheading = element.text().strip()\n",
    "                            current_nested_subheading = None\n",
    "                            data[current_subheading] = {\n",
    "                                \"content\": \"\", \"nested\": {}}\n",
    "                        elif element.filter_by_font(\"nested_subheading\"):\n",
    "                            if current_subheading:\n",
    "                                current_nested_subheading = element.text().strip()\n",
    "                                data[current_subheading][\"nested\"][current_nested_subheading] = \"\"\n",
    "                        elif element.filter_by_font(\"content\"):\n",
    "                            if current_nested_subheading:\n",
    "                                data[current_subheading][\"nested\"][current_nested_subheading] += \" \" + \\\n",
    "                                    element.text().strip()\n",
    "                            elif current_subheading:\n",
    "                                data[current_subheading][\"content\"] += \" \" + \\\n",
    "                                    element.text().strip()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping element due to exception: {str(e)}\")\n",
    "                output_data = {\n",
    "                    \"title_policy\": title_policy,\n",
    "                    \"content\": data\n",
    "                }\n",
    "                os.makedirs(self.output_folder, exist_ok=True)\n",
    "                output_file_path = os.path.join(\n",
    "                    self.output_folder, f\"{os.path.basename(file_path)}.json\")\n",
    "\n",
    "                with open(output_file_path, \"w\") as json_file:\n",
    "                    json.dump(output_data, json_file, indent=4)\n",
    "\n",
    "                return f\"File saved successfully at {output_file_path}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error : {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def parse_text_with_pypdf2(self, file_path):\n",
    "        try:\n",
    "            os.makedirs(self.output_folder, exist_ok=True)\n",
    "\n",
    "            filenames = os.listdir(file_path)\n",
    "            for filename in filenames:\n",
    "                input_file_path = os.path.join(file_path, filename)\n",
    "                document = PdfReader(input_file_path)\n",
    "                num_pages = len(document.pages)\n",
    "\n",
    "                output_file_name = f\"{os.path.splitext(filename)[0]}.txt\"\n",
    "                output_file_path = os.path.join(\n",
    "                    self.output_folder, output_file_name)\n",
    "\n",
    "                with open(output_file_path, \"w\", encoding='utf-8') as output_file:\n",
    "                    text = ''\n",
    "                    for page in document.pages:\n",
    "                        text += page.extract_text()\n",
    "                    output_file.write(text)\n",
    "\n",
    "            return f\"All files processed and saved in {self.output_folder}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing files with PyPDF2: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "class TableExtractor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.output_folder = config.get('output_folder', 'output_tables')\n",
    "        load_dotenv()\n",
    "\n",
    "    def get_table_parsing_module(self):\n",
    "        if self.config['table_parsing'] == 'camelot':\n",
    "            return self.parse_tables_with_camelot\n",
    "        elif self.config['table_parsing'] == 'llm':\n",
    "            return self.parse_tables_with_llm\n",
    "\n",
    "    def parse_tables_with_camelot(self, file_path):\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "\n",
    "        tables = camelot.read_pdf(\n",
    "            file_path, pages='all', flavor=\"lattice\", suppress_stdout=True)\n",
    "        print(f\"Total tables extracted: {len(tables)}\")\n",
    "\n",
    "        csv_dir = os.path.join(self.output_folder, 'csv_files')\n",
    "        os.makedirs(csv_dir, exist_ok=True)\n",
    "        for i, table in enumerate(tables):\n",
    "            output_path = os.path.join(csv_dir, f'table_{i}.csv')\n",
    "            table.to_csv(output_path)\n",
    "        tables_dict = self.export_csv_tables_to_dict(csv_dir)\n",
    "        json_output_path = os.path.join(\n",
    "            self.output_folder, 'tables_output.json')\n",
    "        self.save_dict_to_json(tables_dict, json_output_path)\n",
    "        print(f\"Tables saved as JSON in {json_output_path}\")\n",
    "\n",
    "        return json_output_path\n",
    "\n",
    "    def csv_to_dict(self, file_path):\n",
    "        table_dict = {}\n",
    "        with open(file_path, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                for column, value in row.items():\n",
    "                    if column not in table_dict:\n",
    "                        table_dict[column] = []\n",
    "                    table_dict[column].append(value)\n",
    "        # Convert lists to comma-separated strings\n",
    "        for column in table_dict:\n",
    "            table_dict[column] = ','.join(table_dict[column])\n",
    "        return table_dict\n",
    "\n",
    "    def export_csv_tables_to_dict(self, directory):\n",
    "        tables_dict = {}\n",
    "        table_number = 1\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                tables_dict[table_number] = self.csv_to_dict(file_path)\n",
    "                table_number += 1\n",
    "        return tables_dict\n",
    "\n",
    "    def save_dict_to_json(self, data, file_path):\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "\n",
    "    def parse_tables_with_llm(self, file_path):\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "\n",
    "        api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\n",
    "                \"LLAMA_CLOUD_API_KEY environment variable is not set\")\n",
    "        document = LlamaParse(result_type=\"markdown\").load_data(file_path)\n",
    "        output_file = os.path.join(self.output_folder, 'llm_parsed_tables.txt')\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            for i in range(len(document)):\n",
    "                excerpt = document[i].text[:1000]\n",
    "                file.write(f\"Document {i+1}:\\n\")\n",
    "                file.write(excerpt)\n",
    "                # Separator between documents\n",
    "                file.write(\"\\n\\n\" + \"-\"*50 + \"\\n\\n\")\n",
    "            print(f\"Tables have been saved to {output_file}\")\n",
    "        return output_file\n",
    "\n",
    "\n",
    "class QAPairBuilder:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def build_qa_pairs(self, parsed_text, parsed_tables):\n",
    "        return [(\"Sample Question\", \"Sample Answer\")]\n",
    "\n",
    "\n",
    "class ModelFineTuner:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def fine_tune_model(self, qa_pairs):\n",
    "        print(f\"Fine-tuning model with {len(qa_pairs)} Q&A pairs\")\n",
    "\n",
    "\n",
    "class DTDLBotPipeline:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.text_extractor = TextExtractor(config)\n",
    "        self.table_extractor = TableExtractor(config)\n",
    "        self.qa_pair_builder = QAPairBuilder(config)\n",
    "        self.model_fine_tuner = ModelFineTuner(config)\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            parsed_text = self.text_extractor.get_text_parsing_module()\n",
    "            parsed_tables = self.table_extractor.get_table_parsing_module()\n",
    "            qa_pairs = self.qa_pair_builder.build_qa_pairs(\n",
    "                parsed_text, parsed_tables)\n",
    "            self.model_fine_tuner.fine_tune_model(qa_pairs)\n",
    "            print(f\"Pipeline run successfully with config: {self.config}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error running pipeline: {str(e)}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "config = {\n",
    "    'pdf_path': 'path/to/your/pdf',\n",
    "    'text_parsing': 'pypdf2',\n",
    "    'table_parsing': 'camelot'\n",
    "}\n",
    "\n",
    "pipeline = DTDLBotPipeline(config)\n",
    "pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
